## CS 224N: Natural Language Processing with Deep Learning

# Compositional Pre-Training for Semantic Parsing with BERT

#### Arnaud Autef, Simon Hagege

Semantic parsing - the conversion of natural language utterances to logical forms - is a typical natural language processing technique. Its applications cover a wide variety of tasks, such as question answering, machine translation, instruction following or regular expression generation. In our project, we investigate the performance of Transformer-based Encoder-Decoder models for semantic parsing. We compare a simple Transformer Encoder Decoder model built on the work of (Vaswani et al. 2017) and a Encoder Decoder model where the Encoder is BERT, a Transformer with weights pre-trained to learn a bidirectional language model over a large dataset. Our architectures will be trained on the semantic parsing data using data recombination techniques described in (Jia et al. 2013). The main contribution of our work is the use of BERT and transformer-based structures in the semantic parsing setting and the analysis of their potential benefits on such tasks. We are mentored by Robin Jia from Stanford University?s Computer Science Department.